import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, plot_tree
from mlxtend.plotting import plot_decision_regions

# Step 1: Create dataset
df = pd.DataFrame()
df['X1'] = [1, 2, 3, 4, 5, 6, 6, 7, 9, 9]
df['X2'] = [5, 3, 6, 8, 1, 9, 5, 8, 9, 2]
df['label'] = [1, 1, 0, 1, 0, 1, 0, 1, 0, 0]
df['weights'] = 1 / df.shape[0]

# Visualize initial data
sns.scatterplot(x='X1', y='X2', hue='label', data=df)
plt.title("Initial Data")
plt.show()

# Step 2: Train first decision stump
x = df[['X1', 'X2']].values
y = df['label'].values
dt1 = DecisionTreeClassifier(max_depth=1)
dt1.fit(x, y)
df['y_pred'] = dt1.predict(x)

plot_decision_regions(x, y.astype(int), clf=dt1, legend=2)
plt.title("Decision Region - Tree 1")
plt.show()

# Step 3: Calculate model weight
def calculate_model_weight(error, epsilon=1e-10):
    error = np.clip(error, epsilon, 1 - epsilon)
    return 0.5 * np.log((1 - error) / error)
error1 = np.sum(df['weights'] * (df['label'] != df['y_pred']))
alpha1 = calculate_model_weight(error1)

# Step 4: Update weights
def update_row_weights(row, alpha):
    if row['label'] == row['y_pred']:
      return row['weights'] * np.exp(-alpha)
    else:
      return row['weights'] * np.exp(alpha)
df['updated_weights'] = df.apply(update_row_weights, axis=1, alpha=alpha1)
df['normalized_weights'] = df['updated_weights'] / df['updated_weights'].sum()
df['cumsum_upper'] = np.cumsum(df['normalized_weights'])
df['cumsum_lower'] = df['cumsum_upper'] - df['normalized_weights']

# Step 5: Resample based on weights
def create_new_dataset(df):
  indices = []
  for _ in range(df.shape[0]):
    a = np.random.random()
    for index, row in df.iterrows():
      if row['cumsum_lower'] < a <= row['cumsum_upper']:
        indices.append(index)
        break
  return indices
index_values = create_new_dataset(df)
second_df = df.iloc[index_values].copy()

# Step 6: Train second decision stump
x2 = second_df[['X1', 'X2']].values
y2 = second_df['label'].values
dt2 = DecisionTreeClassifier(max_depth=1)
dt2.fit(x2, y2)
second_df['y_pred'] = dt2.predict(x2)

# Plot decision region
plot_decision_regions(x2, y2.astype(int), clf=dt2, legend=2)
plt.title("Decision Region - Tree 2")
plt.show()

# Step 7: Calculate second model weight
error2 = np.sum(second_df['weights'] * (second_df['label'] != second_df['y_pred']))
alpha2 = calculate_model_weight(error2)
# Step 8: Train third decision stump on original data
dt3 = DecisionTreeClassifier(max_depth=1)
dt3.fit(x, y)
df['y_pred_3'] = dt3.predict(x)
error3 = np.sum(df['weights'] * (df['label'] != df['y_pred_3']))
alpha3 = calculate_model_weight(error3)

# Print model weights
print("Alpha1:", alpha1)
print("Alpha2:", alpha2)
print("Alpha3:", alpha3)

# Step 9: Ensemble prediction for query point
def ensemble_predict(query):
  pred1 = dt1.predict(query)[0]
  pred2 = dt2.predict(query)[0]
  pred3 = dt3.predict(query)[0]
  vote = (alpha1 * (1 if pred1 == 1 else -1) +alpha2 * (1 if pred2 == 1 else -1) +alpha3 * (1 if pred3 == 1 else -1))
  return int(np.sign(vote) > 0)

# Test queries
query1 = np.array([1, 5]).reshape(1, -1)
query2 = np.array([9, 9]).reshape(1, -1)

print("Ensemble prediction for [1,5]:", ensemble_predict(query1))
print("Ensemble prediction for [9,9]:", ensemble_predict(query2))
